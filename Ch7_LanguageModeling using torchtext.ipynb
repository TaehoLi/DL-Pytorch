{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torchtext import data as d\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = d.Field(lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make splits for data\n",
    "train, valid, test = datasets.WikiText2.splits(TEXT,root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "bptt_len=30\n",
    "clip = 0.25\n",
    "lr = 20\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(valid[0].text)//batch_size)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].text = train[0].text[:(len(train[0].text)//batch_size)*batch_size]\n",
    "valid[0].text = valid[0].text[:(len(valid[0].text)//batch_size)*batch_size]\n",
    "test[0].text = test[0].text[:(len(valid[0].text)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f85fda2d5f8>}\n",
      "len(train) 1\n",
      "vars(train[0]) ['<eos>', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', '<eos>', 'senjÅ', 'no']\n"
     ]
    }
   ],
   "source": [
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0])['text'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 28913\n"
     ]
    }
   ],
   "source": [
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = d.BPTTIterator.splits((train, valid, test), batch_size=batch_size, bptt_len=bptt_len, device='cuda', repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,ntoken,ninp,nhid,nlayers,dropout=0.5,tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout()\n",
    "        self.encoder = nn.Embedding(ntoken,ninp)\n",
    "        self.rnn = nn.LSTM(ninp,nhid,nlayers,dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange,initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "    def forward(self,input,hidden): \n",
    "        \n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output,hidden = self.rnn(emb,hidden)\n",
    "        output = self.drop(output)\n",
    "        s = output.size()\n",
    "        decoded = self.decoder(output.view(s[0]*s[1],s[2]))\n",
    "        return decoded.view(s[0],s[1],decoded.size(1)),hidden\n",
    "    \n",
    "    def init_hidden(self,bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return(Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()),Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217640"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_iter.dataset[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 200\n",
    "nhid=200\n",
    "nlayers=2\n",
    "dropout = 0.2\n",
    "\n",
    "ntokens = len(TEXT.vocab)\n",
    "lstm = RNNModel(ntokens, emsize, nhid,nlayers, dropout, 'store_true')\n",
    "if is_cuda:\n",
    "    lstm = lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Tensor:\n",
    "        return h.detach().cuda()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    lstm.eval()\n",
    "    total_loss = 0   \n",
    "    hidden = lstm.init_hidden(batch_size)\n",
    "    for batch in data_source:        \n",
    "        data, targets = batch.text,batch.target.view(-1)\n",
    "        output, hidden = lstm(data.cuda(), hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        \n",
    "        if is_cuda :\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item()/(len(data_source.dataset[0].text)//batch_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainf():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    lstm.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = lstm.init_hidden(batch_size)\n",
    "    for  i,batch in enumerate(train_iter):\n",
    "        data, targets = batch.text,batch.target.view(-1)\n",
    "        if is_cuda :\n",
    "            data = data.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        lstm.zero_grad()\n",
    "        output, hidden = lstm(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm_` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), clip)\n",
    "        for p in lstm.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'\n",
    "                   .format(epoch, i, len(train_iter), lr,elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/ 3481 batches | lr 20.00 | ms/batch 54.50 | loss  6.57 | ppl   713.56\n",
      "| epoch   1 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.79 | loss  5.94 | ppl   381.49\n",
      "| epoch   1 |  3000/ 3481 batches | lr 20.00 | ms/batch 55.17 | loss  5.74 | ppl   311.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 194.84s | valid loss  5.43 |valid ppl   227.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 3481 batches | lr 20.00 | ms/batch 55.42 | loss  5.56 | ppl   260.44\n",
      "| epoch   2 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.86 | loss  5.49 | ppl   243.14\n",
      "| epoch   2 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.82 | loss  5.43 | ppl   228.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 195.39s | valid loss  5.21 |valid ppl   183.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  1000/ 3481 batches | lr 20.00 | ms/batch 55.08 | loss  5.36 | ppl   213.71\n",
      "| epoch   3 |  2000/ 3481 batches | lr 20.00 | ms/batch 55.00 | loss  5.34 | ppl   208.33\n",
      "| epoch   3 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.11 | loss  5.30 | ppl   201.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 194.68s | valid loss  5.14 |valid ppl   170.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  1000/ 3481 batches | lr 20.00 | ms/batch 54.85 | loss  5.27 | ppl   193.67\n",
      "| epoch   4 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.51 | loss  5.26 | ppl   192.08\n",
      "| epoch   4 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.72 | loss  5.23 | ppl   186.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 194.30s | valid loss  5.08 |valid ppl   161.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |  1000/ 3481 batches | lr 20.00 | ms/batch 55.41 | loss  5.20 | ppl   181.33\n",
      "| epoch   5 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.79 | loss  5.20 | ppl   181.98\n",
      "| epoch   5 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.97 | loss  5.18 | ppl   177.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 195.52s | valid loss  5.05 |valid ppl   155.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |  1000/ 3481 batches | lr 20.00 | ms/batch 55.18 | loss  5.15 | ppl   173.03\n",
      "| epoch   6 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.85 | loss  5.16 | ppl   174.52\n",
      "| epoch   6 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.73 | loss  5.14 | ppl   170.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 194.53s | valid loss  5.02 |valid ppl   151.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |  1000/ 3481 batches | lr 20.00 | ms/batch 54.43 | loss  5.12 | ppl   167.05\n",
      "| epoch   7 |  2000/ 3481 batches | lr 20.00 | ms/batch 54.52 | loss  5.13 | ppl   169.08\n",
      "| epoch   7 |  3000/ 3481 batches | lr 20.00 | ms/batch 53.90 | loss  5.11 | ppl   165.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 192.61s | valid loss  4.99 |valid ppl   146.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |  1000/ 3481 batches | lr 20.00 | ms/batch 56.72 | loss  5.09 | ppl   161.80\n",
      "| epoch   8 |  2000/ 3481 batches | lr 20.00 | ms/batch 55.07 | loss  5.10 | ppl   164.70\n",
      "| epoch   8 |  3000/ 3481 batches | lr 20.00 | ms/batch 54.69 | loss  5.08 | ppl   161.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 196.76s | valid loss  4.97 |valid ppl   143.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |  1000/ 3481 batches | lr 20.00 | ms/batch 55.04 | loss  5.07 | ppl   158.53\n",
      "| epoch   9 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.97 | loss  5.08 | ppl   161.07\n",
      "| epoch   9 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  5.07 | ppl   158.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 101.72s | valid loss  4.95 |valid ppl   141.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.25 | loss  5.04 | ppl   155.05\n",
      "| epoch  10 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  5.06 | ppl   157.88\n",
      "| epoch  10 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.98 | loss  5.04 | ppl   155.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 64.95s | valid loss  4.94 |valid ppl   139.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.23 | loss  5.02 | ppl   152.08\n",
      "| epoch  11 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.94 | loss  5.04 | ppl   155.24\n",
      "| epoch  11 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.94 | loss  5.03 | ppl   152.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 64.86s | valid loss  4.93 |valid ppl   138.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.22 | loss  5.01 | ppl   149.63\n",
      "| epoch  12 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.94 | loss  5.03 | ppl   153.47\n",
      "| epoch  12 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  5.01 | ppl   150.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 64.86s | valid loss  4.92 |valid ppl   137.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.23 | loss  4.99 | ppl   146.75\n",
      "| epoch  13 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.95 | loss  5.02 | ppl   150.80\n",
      "| epoch  13 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.95 | loss  5.00 | ppl   147.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 64.89s | valid loss  4.92 |valid ppl   136.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.24 | loss  4.98 | ppl   145.12\n",
      "| epoch  14 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  5.00 | ppl   149.00\n",
      "| epoch  14 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  4.99 | ppl   146.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 64.91s | valid loss  4.91 |valid ppl   135.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |  1000/ 3481 batches | lr 20.00 | ms/batch 18.23 | loss  4.96 | ppl   143.28\n",
      "| epoch  15 |  2000/ 3481 batches | lr 20.00 | ms/batch 17.96 | loss  4.99 | ppl   147.00\n",
      "| epoch  15 |  3000/ 3481 batches | lr 20.00 | ms/batch 17.95 | loss  4.97 | ppl   144.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 64.88s | valid loss  4.91 |valid ppl   135.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.19 | loss  4.96 | ppl   143.24\n",
      "| epoch  16 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.93 | loss  4.93 | ppl   138.06\n",
      "| epoch  16 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.93 | loss  4.86 | ppl   129.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 64.78s | valid loss  4.80 |valid ppl   121.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.20 | loss  4.88 | ppl   131.32\n",
      "| epoch  17 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.91 | loss  4.88 | ppl   131.26\n",
      "| epoch  17 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.93 | loss  4.83 | ppl   125.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 64.78s | valid loss  4.79 |valid ppl   119.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.17 | loss  4.84 | ppl   127.09\n",
      "| epoch  18 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.90 | loss  4.85 | ppl   127.89\n",
      "| epoch  18 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.91 | loss  4.82 | ppl   123.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 64.71s | valid loss  4.78 |valid ppl   118.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.18 | loss  4.82 | ppl   124.49\n",
      "| epoch  19 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.83 | ppl   125.68\n",
      "| epoch  19 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.80 | ppl   121.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 64.75s | valid loss  4.77 |valid ppl   117.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.20 | loss  4.81 | ppl   122.41\n",
      "| epoch  20 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.82 | ppl   124.40\n",
      "| epoch  20 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.79 | ppl   120.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 64.77s | valid loss  4.76 |valid ppl   117.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.19 | loss  4.79 | ppl   120.86\n",
      "| epoch  21 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.81 | ppl   123.06\n",
      "| epoch  21 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.94 | loss  4.79 | ppl   119.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 64.80s | valid loss  4.76 |valid ppl   116.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.20 | loss  4.78 | ppl   119.63\n",
      "| epoch  22 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.93 | loss  4.80 | ppl   121.86\n",
      "| epoch  22 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.78 | ppl   118.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 64.79s | valid loss  4.76 |valid ppl   116.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.22 | loss  4.78 | ppl   118.66\n",
      "| epoch  23 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.93 | loss  4.79 | ppl   120.69\n",
      "| epoch  23 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.94 | loss  4.77 | ppl   117.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 64.82s | valid loss  4.75 |valid ppl   115.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.23 | loss  4.76 | ppl   117.15\n",
      "| epoch  24 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.95 | loss  4.79 | ppl   119.95\n",
      "| epoch  24 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.95 | loss  4.76 | ppl   116.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 64.87s | valid loss  4.75 |valid ppl   115.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |  1000/ 3481 batches | lr 5.00 | ms/batch 18.20 | loss  4.76 | ppl   116.52\n",
      "| epoch  25 |  2000/ 3481 batches | lr 5.00 | ms/batch 17.92 | loss  4.78 | ppl   118.80\n",
      "| epoch  25 |  3000/ 3481 batches | lr 5.00 | ms/batch 17.94 | loss  4.76 | ppl   116.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 64.80s | valid loss  4.75 |valid ppl   115.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.20 | loss  4.78 | ppl   119.60\n",
      "| epoch  26 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.93 | loss  4.78 | ppl   119.08\n",
      "| epoch  26 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.94 | loss  4.74 | ppl   114.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 64.82s | valid loss  4.71 |valid ppl   111.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.20 | loss  4.76 | ppl   116.48\n",
      "| epoch  27 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.93 | loss  4.77 | ppl   117.35\n",
      "| epoch  27 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.95 | loss  4.73 | ppl   113.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 64.84s | valid loss  4.71 |valid ppl   110.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.23 | loss  4.75 | ppl   115.36\n",
      "| epoch  28 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.94 | loss  4.76 | ppl   116.71\n",
      "| epoch  28 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.95 | loss  4.73 | ppl   112.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 64.88s | valid loss  4.71 |valid ppl   110.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.21 | loss  4.74 | ppl   114.59\n",
      "| epoch  29 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.95 | loss  4.75 | ppl   115.94\n",
      "| epoch  29 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.97 | loss  4.72 | ppl   112.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 64.89s | valid loss  4.71 |valid ppl   110.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.23 | loss  4.74 | ppl   114.17\n",
      "| epoch  30 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.94 | loss  4.75 | ppl   115.53\n",
      "| epoch  30 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.95 | loss  4.72 | ppl   112.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 64.87s | valid loss  4.70 |valid ppl   110.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.24 | loss  4.73 | ppl   113.27\n",
      "| epoch  31 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.95 | loss  4.75 | ppl   115.06\n",
      "| epoch  31 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.96 | loss  4.72 | ppl   112.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 64.90s | valid loss  4.70 |valid ppl   110.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.21 | loss  4.73 | ppl   113.09\n",
      "| epoch  32 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.92 | loss  4.75 | ppl   115.03\n",
      "| epoch  32 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.94 | loss  4.72 | ppl   111.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 64.83s | valid loss  4.70 |valid ppl   110.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |  1000/ 3481 batches | lr 1.25 | ms/batch 18.23 | loss  4.73 | ppl   112.75\n",
      "| epoch  33 |  2000/ 3481 batches | lr 1.25 | ms/batch 17.94 | loss  4.74 | ppl   114.45\n",
      "| epoch  33 |  3000/ 3481 batches | lr 1.25 | ms/batch 17.96 | loss  4.71 | ppl   111.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 64.88s | valid loss  4.70 |valid ppl   110.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.23 | loss  4.75 | ppl   115.24\n",
      "| epoch  34 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.94 | loss  4.75 | ppl   115.52\n",
      "| epoch  34 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.95 | loss  4.71 | ppl   111.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 64.88s | valid loss  4.69 |valid ppl   108.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.24 | loss  4.73 | ppl   113.84\n",
      "| epoch  35 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.96 | loss  4.74 | ppl   114.73\n",
      "| epoch  35 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.96 | loss  4.71 | ppl   111.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 64.90s | valid loss  4.69 |valid ppl   108.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.23 | loss  4.73 | ppl   113.45\n",
      "| epoch  36 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.95 | loss  4.74 | ppl   114.76\n",
      "| epoch  36 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.95 | loss  4.71 | ppl   111.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 64.89s | valid loss  4.69 |valid ppl   108.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.23 | loss  4.73 | ppl   113.29\n",
      "| epoch  37 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.94 | loss  4.74 | ppl   114.31\n",
      "| epoch  37 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.96 | loss  4.71 | ppl   110.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 64.87s | valid loss  4.69 |valid ppl   108.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.24 | loss  4.73 | ppl   112.90\n",
      "| epoch  38 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.97 | loss  4.74 | ppl   114.27\n",
      "| epoch  38 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.97 | loss  4.71 | ppl   110.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 64.94s | valid loss  4.68 |valid ppl   108.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |  1000/ 3481 batches | lr 0.31 | ms/batch 18.22 | loss  4.73 | ppl   112.81\n",
      "| epoch  39 |  2000/ 3481 batches | lr 0.31 | ms/batch 17.94 | loss  4.74 | ppl   114.03\n",
      "| epoch  39 |  3000/ 3481 batches | lr 0.31 | ms/batch 17.94 | loss  4.71 | ppl   110.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 64.85s | valid loss  4.68 |valid ppl   108.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |  1000/ 3481 batches | lr 0.08 | ms/batch 18.24 | loss  4.73 | ppl   113.70\n",
      "| epoch  40 |  2000/ 3481 batches | lr 0.08 | ms/batch 17.96 | loss  4.74 | ppl   114.70\n",
      "| epoch  40 |  3000/ 3481 batches | lr 0.08 | ms/batch 17.96 | loss  4.70 | ppl   110.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 64.90s | valid loss  4.68 |valid ppl   107.94\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    trainf()\n",
    "    val_loss = evaluate(valid_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} |'\n",
    "          'valid ppl {:8.2f}' .format(epoch, (time.time()-epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
