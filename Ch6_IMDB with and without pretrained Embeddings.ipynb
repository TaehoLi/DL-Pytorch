{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data,datasets\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchtext.datasets.imdb import IMDB\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True,fix_length=40)\n",
    "LABEL = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.58MB/s]\n"
     ]
    }
   ],
   "source": [
    "train, test = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.datasets.imdb.IMDB"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7fc2504bd048>, 'label': <torchtext.data.field.Field object at 0x7fc250497f28>}\n",
      "len(train) 25000\n",
      "vars(train[0]) {'text': ['at', 'first,this', 'movie', 'seems', 'so', 'bad', 'that', 'i', 'almost', 'fell', 'in', 'a', 'trance', 'the', 'first', 'time', 'i', 'saw', 'it.it', 'was', 'like', 'a', 'bad', 'dream.a', 'cosmic', 'bore.but', 'i', 'gave', 'it', 'a', 'second', 'chance,then', 'another', 'and', 'another,etc...i', 'finally', 'got', 'addicted', 'to', 'this', 'film,due', 'to', \"it's\", 'dreamlike', 'slow', 'pace,wonderful', 'natural', 'sets,bathed', 'in', 'a', 'mellow', 'autumn', 'light', 'and', 'especially', 'the', 'musical', 'score,which', 'is', 'made', 'of', 'some', \"70's\", 'progressive', 'rock', 'and', 'absolute', 'exquisite', 'folk', 'songs', 'by', 'actor/singer/songwriter', 'derek', 'lamb(the', 'troubadour).you', 'should', 'notice', 'the', 'song', 'about', 'hazel', 'wood,silver', 'trout', 'and', 'lady', 'vanishing', 'in', 'the', 'air...,heard', 'in', 'the', 'middle', 'and', 'near', 'the', 'end', 'of', 'the', 'film.there', 'are', 'some', 'carnal', 'scenes', 'in', 'the', 'beginning', ',wich', 'allow', 'us', 'to', 'appreciate', 'the', 'natural', 'charms', 'of', 'elizabeth', 'suzuki.if', 'that', 'movie', 'had', 'been', 'made', 'by', 'some', '\"repertoire\"', 'directors', 'like', 'bergman,lars', 'von', 'triers', 'or', 'jean-luc', 'goddard,critics', 'would', 'have', 'rolled', 'on', 'the', 'floor,raving', 'about', 'that', 'movie', 'as', 'if', 'it', 'were', 'a', 'cosmic', 'masterpiece.i', 'personally', 'think', 'this', 'film', 'is', 'one', 'million', 'times', 'superior', 'to', 'any', 'of', \"fellini's\", 'cinematic', 'sh#¤@t!definitely', 'not', 'for', 'the', 'pretentious.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [03:51, 3.73MB/s]                           \n",
      "100%|█████████▉| 399089/400000 [00:28<00:00, 14456.36it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pos': 12500, 'neg': 12500})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7fc2504bd048>, 'label': <torchtext.data.field.Field object at 0x7fc250497f28>}\n",
      "len(train) 25000\n",
      "vars(train[0]) {'text': ['at', 'first,this', 'movie', 'seems', 'so', 'bad', 'that', 'i', 'almost', 'fell', 'in', 'a', 'trance', 'the', 'first', 'time', 'i', 'saw', 'it.it', 'was', 'like', 'a', 'bad', 'dream.a', 'cosmic', 'bore.but', 'i', 'gave', 'it', 'a', 'second', 'chance,then', 'another', 'and', 'another,etc...i', 'finally', 'got', 'addicted', 'to', 'this', 'film,due', 'to', \"it's\", 'dreamlike', 'slow', 'pace,wonderful', 'natural', 'sets,bathed', 'in', 'a', 'mellow', 'autumn', 'light', 'and', 'especially', 'the', 'musical', 'score,which', 'is', 'made', 'of', 'some', \"70's\", 'progressive', 'rock', 'and', 'absolute', 'exquisite', 'folk', 'songs', 'by', 'actor/singer/songwriter', 'derek', 'lamb(the', 'troubadour).you', 'should', 'notice', 'the', 'song', 'about', 'hazel', 'wood,silver', 'trout', 'and', 'lady', 'vanishing', 'in', 'the', 'air...,heard', 'in', 'the', 'middle', 'and', 'near', 'the', 'end', 'of', 'the', 'film.there', 'are', 'some', 'carnal', 'scenes', 'in', 'the', 'beginning', ',wich', 'allow', 'us', 'to', 'appreciate', 'the', 'natural', 'charms', 'of', 'elizabeth', 'suzuki.if', 'that', 'movie', 'had', 'been', 'made', 'by', 'some', '\"repertoire\"', 'directors', 'like', 'bergman,lars', 'von', 'triers', 'or', 'jean-luc', 'goddard,critics', 'would', 'have', 'rolled', 'on', 'the', 'floor,raving', 'about', 'that', 'movie', 'as', 'if', 'it', 'were', 'a', 'cosmic', 'masterpiece.i', 'personally', 'think', 'this', 'film', 'is', 'one', 'million', 'times', 'superior', 'to', 'any', 'of', \"fellini's\", 'cinematic', 'sh#¤@t!definitely', 'not', 'for', 'the', 'pretentious.'], 'label': 'pos'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399089/400000 [00:40<00:00, 14456.36it/s]"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = vars(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7724, -0.1800,  0.2072,  ...,  0.6736,  0.2263, -0.2919],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device='cpu')\n",
    "\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbNet(nn.Module):\n",
    "    def __init__(self,emb_size,hidden_size1,hidden_size2=400):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(emb_size,hidden_size1)\n",
    "        self.fc = nn.Linear(hidden_size2,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeds = self.embedding(x).view(x.size(0),-1)\n",
    "        out = self.fc(embeds)\n",
    "        return F.log_softmax(out,dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbNet(len(TEXT.vocab.stoi),10)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device='cpu', shuffle=True)\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch,model,data_loader,phase='training',volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile=True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for batch_idx , batch in enumerate(data_loader):\n",
    "        text , target = batch.text , batch.label\n",
    "        if is_cuda:\n",
    "            text,target = text.cuda(),target.cuda()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        \n",
    "        running_loss += F.nll_loss(output,target, reduction='sum').data\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct.item()/len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses , train_accuracy = [],[]\n",
    "val_losses , val_accuracy = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.74 and training accuracy is 12866/25000     51.46\n",
      "validation loss is   0.7 and validation accuracy is 13510/25000     54.04\n",
      "training loss is  0.68 and training accuracy is 14360/25000     57.44\n",
      "validation loss is  0.68 and validation accuracy is 14579/25000     58.32\n",
      "training loss is  0.64 and training accuracy is 15774/25000      63.1\n",
      "validation loss is  0.65 and validation accuracy is 15587/25000     62.35\n",
      "training loss is   0.6 and training accuracy is 16992/25000     67.97\n",
      "validation loss is  0.62 and validation accuracy is 16436/25000     65.74\n",
      "training loss is  0.55 and training accuracy is 17849/25000      71.4\n",
      "validation loss is  0.61 and validation accuracy is 16859/25000     67.44\n",
      "training loss is  0.51 and training accuracy is 18512/25000     74.05\n",
      "validation loss is   0.6 and validation accuracy is 17181/25000     68.72\n",
      "training loss is  0.48 and training accuracy is 19112/25000     76.45\n",
      "validation loss is   0.6 and validation accuracy is 17374/25000      69.5\n",
      "training loss is  0.46 and training accuracy is 19566/25000     78.26\n",
      "validation loss is   0.6 and validation accuracy is 17538/25000     70.15\n",
      "training loss is  0.43 and training accuracy is 19994/25000     79.98\n",
      "validation loss is   0.6 and validation accuracy is 17536/25000     70.14\n",
      "CPU times: user 12.7 s, sys: 232 ms, total: 12.9 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses , train_accuracy = [],[]\n",
    "val_losses , val_accuracy = [],[]\n",
    "\n",
    "for epoch in range(1,10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 학습 Glove 워드 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True,fix_length=40)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train, test = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "TEXT.build_vocab(train, test, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbNet(nn.Module):\n",
    "    def __init__(self,emb_size,hidden_size1,hidden_size2=400):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(emb_size,hidden_size1)\n",
    "        self.fc1 = nn.Linear(hidden_size2,3)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeds = self.embedding(x).view(x.size(0),-1)\n",
    "        out = self.fc1(embeds)\n",
    "        return F.log_softmax(out,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbNet(len(TEXT.vocab.stoi),300,12000)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = TEXT.vocab.vectors.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "optimizer = optim.Adam([ param for param in model.parameters() if param.requires_grad == True],lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=64, device='cpu',shuffle=True)\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch,model,data_loader,phase='training',volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile=True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for batch_idx , batch in enumerate(data_loader):\n",
    "        text , target = batch.text , batch.label\n",
    "        if is_cuda:\n",
    "            text,target = text.cuda(),target.cuda()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        \n",
    "        running_loss += F.nll_loss(output,target, reduction='sum').data\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct.item()/len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.66 and training accuracy is 15675/25000      62.7\n",
      "validation loss is  0.68 and validation accuracy is 16011/25000     64.04\n",
      "training loss is  0.56 and training accuracy is 17788/25000     71.15\n",
      "validation loss is  0.65 and validation accuracy is 16526/25000      66.1\n",
      "training loss is  0.53 and training accuracy is 18334/25000     73.34\n",
      "validation loss is  0.69 and validation accuracy is 16326/25000      65.3\n",
      "training loss is  0.51 and training accuracy is 18706/25000     74.82\n",
      "validation loss is  0.71 and validation accuracy is 16275/25000      65.1\n",
      "training loss is  0.49 and training accuracy is 19012/25000     76.05\n",
      "validation loss is  0.72 and validation accuracy is 16286/25000     65.14\n",
      "training loss is  0.48 and training accuracy is 19201/25000      76.8\n",
      "validation loss is  0.74 and validation accuracy is 16205/25000     64.82\n",
      "training loss is  0.47 and training accuracy is 19487/25000     77.95\n",
      "validation loss is  0.75 and validation accuracy is 16319/25000     65.28\n",
      "training loss is  0.46 and training accuracy is 19511/25000     78.04\n",
      "validation loss is  0.78 and validation accuracy is 16133/25000     64.53\n",
      "training loss is  0.45 and training accuracy is 19606/25000     78.42\n",
      "validation loss is  0.78 and validation accuracy is 16291/25000     65.16\n",
      "CPU times: user 8.35 s, sys: 97.4 ms, total: 8.45 s\n",
      "Wall time: 8.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1,10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.44 and training accuracy is 19816/25000     79.26\n",
      "validation loss is  0.79 and validation accuracy is 16175/25000      64.7\n",
      "training loss is  0.44 and training accuracy is 19842/25000     79.37\n",
      "validation loss is  0.82 and validation accuracy is 16073/25000     64.29\n",
      "training loss is  0.44 and training accuracy is 19830/25000     79.32\n",
      "validation loss is  0.83 and validation accuracy is 16129/25000     64.52\n",
      "training loss is  0.43 and training accuracy is 19929/25000     79.72\n",
      "validation loss is  0.85 and validation accuracy is 16113/25000     64.45\n",
      "training loss is  0.42 and training accuracy is 20022/25000     80.09\n",
      "validation loss is  0.87 and validation accuracy is 16064/25000     64.26\n",
      "training loss is  0.42 and training accuracy is 20128/25000     80.51\n",
      "validation loss is  0.87 and validation accuracy is 15952/25000     63.81\n",
      "training loss is  0.42 and training accuracy is 20179/25000     80.72\n",
      "validation loss is  0.89 and validation accuracy is 16102/25000     64.41\n",
      "training loss is  0.41 and training accuracy is 20234/25000     80.94\n",
      "validation loss is  0.91 and validation accuracy is 16020/25000     64.08\n",
      "training loss is  0.41 and training accuracy is 20235/25000     80.94\n",
      "validation loss is  0.91 and validation accuracy is 15955/25000     63.82\n",
      "CPU times: user 8.28 s, sys: 142 ms, total: 8.42 s\n",
      "Wall time: 8.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1,10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
